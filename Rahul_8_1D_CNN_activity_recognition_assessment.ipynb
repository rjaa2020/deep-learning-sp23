{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-jainr/deep-learning-sp23/blob/main/Rahul_8_1D_CNN_activity_recognition_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bkHwspbXK2s"
      },
      "source": [
        "# Copyright\n",
        "<pre>\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "Copyright (c) 2023 Bálint Gyires-Tóth - All Rights Reserved\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0kz73yNXK2t"
      },
      "source": [
        "In this notebook, we will classify human activities based on smartphone accelerometer and gyroscope data. The corresponding Human Activity Recognition Using Smartphones Data Set is delivered by UCI Machine Learning Repository, for more information [please visit the site](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data acquisition\n",
        "Lets download and unpack the data."
      ],
      "metadata": {
        "id": "zy5lL5OEKvZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv5CJEQfKvFT",
        "outputId": "f555225e-cfa2-4a2f-c0d7-d87cfff65c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-12 18:54:04--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60999314 (58M) [application/x-httpd-php]\n",
            "Saving to: ‘UCI HAR Dataset.zip’\n",
            "\n",
            "UCI HAR Dataset.zip 100%[===================>]  58.17M  16.5MB/s    in 4.6s    \n",
            "\n",
            "2023-04-12 18:54:10 (12.5 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"UCI HAR Dataset.zip\" > null"
      ],
      "metadata": {
        "id": "9_bgc7Y2GTpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Imports\n",
        "In this notebook we will need the following packages."
      ],
      "metadata": {
        "id": "A8IHEttFHzUv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWBNCVa0bf65"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, Conv1D, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loading the data\n",
        "The following functions will load the training and test data from the unpacked dataset."
      ],
      "metadata": {
        "id": "uzBI0j52H_sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function loads a single file (input or output)\n",
        "def load_file(filepath):\n",
        "  df = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "  return df.values\n",
        "\n",
        "# this function loads a set of files into a Numpy array with a shape of [samples, timesteps, features]\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "  loaded = np.dstack(loaded)\n",
        "  return loaded\n",
        "\n",
        " # load inputs and outputs from a directory\n",
        "def load_XY(group, path):\n",
        "  filepath = path + group + '/Inertial Signals/'\n",
        "  filenames = list()\n",
        "  filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt'] + \\\n",
        "               ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt'] + \\\n",
        "               ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "  # inputs\n",
        "  X = load_group(filenames, filepath)\n",
        "  # output (class)\n",
        "  Y = load_file(path + group + '/y_'+group+'.txt')\n",
        "  return X, Y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(path='UCI HAR Dataset/'):\n",
        "  # load all train\n",
        "  X_train, Y_train = load_XY('train', path)\n",
        "  X_test,  Y_test  = load_XY('test', path)\n",
        "  Y_train = to_categorical(Y_train - 1 ) # we need -1 to scale the categories between 0..5 (from 1..6)\n",
        "  Y_test  = to_categorical(Y_test - 1 ) # we need -1 to scale the categories between 0..5 (from 1..6)\n",
        "  return X_train, Y_train, X_test, Y_test"
      ],
      "metadata": {
        "id": "1boApy2lLnhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = load_dataset()"
      ],
      "metadata": {
        "id": "l8hh4QYmMMtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo97vqf3BwRN",
        "outputId": "fc29113e-5224-4504-96bd-b4ecef12f5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7352, 128, 9), (7352, 6), (2947, 128, 9), (2947, 6))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train[0],"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtGQzkLmCKbL",
        "outputId": "7167b750-eba8-4544-afe8-4eef734e512d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 0., 0., 0., 1., 0.], dtype=float32),)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Data exploration: target variable - exercise 1\n",
        "Let's check how many data instances there are for each class:"
      ],
      "metadata": {
        "id": "Js-7BztvQ7Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  print(f\"No. of instance belong to class {sum(Y_train[:,i]):.0f}\")\n",
        "# print(\"No. of instance belong to class 1:\",len(<TODO>))\n",
        "# print(\"No. of instance belong to class 2:\",len(<TODO>))\n",
        "# print(\"No. of instance belong to class 3:\",len(<TODO>))\n",
        "# print(\"No. of instance belong to class 4:\",len(<TODO>))\n",
        "# print(\"No. of instance belong to class 5:\",len(<TODO>))"
      ],
      "metadata": {
        "id": "4PTSOP--jPe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a4965e-5f59-4b6e-f9eb-37130eb9f99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of instance belong to class 1226:\n",
            "No. of instance belong to class 1073:\n",
            "No. of instance belong to class 986:\n",
            "No. of instance belong to class 1286:\n",
            "No. of instance belong to class 1374:\n",
            "No. of instance belong to class 1407:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can doublecheck your calculations by plotting the histogram of the target variables: "
      ],
      "metadata": {
        "id": "VaeV8Nqzjd6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(np.argmax(Y_train,axis=1))"
      ],
      "metadata": {
        "id": "4JTkOAmpRGKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Data exploration: input variables - exercise 2\n",
        "And now let's select one instance from each class and plot the corresponding timeseries:"
      ],
      "metadata": {
        "id": "rVeZhkJ-RB4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 6))\n",
        "\n",
        "for i in range(2): # row\n",
        "  for k in range(3): # column\n",
        "    for l in range(X_train.shape[2]): # dimensons to be displayed\n",
        "      class_no=i*2+k\n",
        "      axs[i,k].plot(<TODO: select the input sequence of one instance from class #class_no>)\n",
        "      axs[i,k].title.set_text(\"Class \"+str(class_no))\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "C_wmgDBDWpFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB0t_uW4XK2z"
      },
      "source": [
        "## 4. Model definition and training - exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U04cYnapXK26"
      },
      "source": [
        "Let's define and train a 1D convolutional neural network, with the multidimensional timeseries as inputs, and the class in one-hot encoding as output. \n",
        "\n",
        "This time your task is to correct the code below and achieve higher than 90% validation accuracy and test accuracy. The test accuracy is calculated in section 5. \n",
        "Currently, **there are many errors (both syntactical and theroetical) in the model definition, compile and fit parts**, that should be fixed to achieve the goal. Don't change the number of layers and the layer types, and validation_split, just the other hyperparameters. There are several good solutions!\n",
        "\n",
        "Hints: \n",
        "* use narrow and deep Conv1D kernels\n",
        "* use standard activation functions for the inner layers (e.g. relu, sigmoid, tanh) with the corresponding initialization method\n",
        "* use the appropriate activation function the output and the corresponding loss function (which is aligned with the classification task)\n",
        "* use regularization, but not too agressive\n",
        "* use standard optimizers\n",
        "* train in mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_filter       = 3   # number of filter/kernels in the convolutional layers\n",
        "filter_length   = 48  # length of the filter/kernel in the convolutional layers\n",
        "window_size     = X_train.shape[1] # the window size defined by the dataset\n",
        "nb_features     = X_train.shape[2] # the number of features of the input data\n",
        "nb_outputs      = Y_train.shape[1] # the number of outputs (defined by the target data)"
      ],
      "metadata": {
        "id": "Yokac5NUR003"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=nb_filter, kernel_size=filter_length, activation='selu', kernel_initializer='he_normal', input_shape=(window_size, nb_features)))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Dropout(0.99))\n",
        "model.add(Conv1D(filters=nb_filter, kernel_size=filter_length, activation='selu', kernel_initializer='he_normal'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Dropout(0.99))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='selu'))\n",
        "model.add(Dropout(0.99))\n",
        "model.add(Dense(nb_outputs, activation='linear'))"
      ],
      "metadata": {
        "id": "N3h94EJTRxSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the model with the appropriate loss function for multiclass classification."
      ],
      "metadata": {
        "id": "CxUOZGFVWZik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse', optimizer='david', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kJ5dVKuzSh-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "692uD8KASsi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(patience=-1, restore_best_weights=True, monitor=\"validation_loss\")"
      ],
      "metadata": {
        "id": "F2K-4bVKSz85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we run the training:"
      ],
      "metadata": {
        "id": "0cT9S6rrbTFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, \n",
        "          epochs=200, batch_size=1, \n",
        "          validation_split=0.2, # don't change the validation split\n",
        "          callbacks=[es],\n",
        "          verbose=2)"
      ],
      "metadata": {
        "id": "JSeo-uv2SnAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXM0IetJSpKM"
      },
      "source": [
        "## 5. Evaluation\n",
        "Please run the cells below to check the test loss and test accuracy (remember, the accuracy must be over 90%) and inspect the other evaluation methods as well. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model.evaluate(X_test,Y_test)\n",
        "print(\"Test loss:\",eval[0])\n",
        "print(\"Test accuracy:\",eval[1])"
      ],
      "metadata": {
        "id": "7sR4-0Rkaz1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "WJDwjNkAaS8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "rkIBcKY9aMdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(Y_test,1),np.argmax(preds,1)))"
      ],
      "metadata": {
        "id": "zrWe3vPJaZ3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf=confusion_matrix(np.argmax(Y_test,1),np.argmax(preds,1))\n",
        "sns.heatmap(conf, annot=True, fmt='d', vmax=100)"
      ],
      "metadata": {
        "id": "kRdfXOypaciv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}